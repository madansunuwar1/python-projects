{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.corpus import words\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from nltk import pos_tag\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that processes the comments and returns clean_comments\n",
    "\n",
    "def text_preprocessing(data):\n",
    "\n",
    "    \n",
    "\n",
    "    #convert to lowercase\n",
    "    data['clean_comments'] = data['comments'].str.lower()\n",
    "\n",
    "    #removing empty columns\n",
    "    data['clean_comments'].replace('', np.nan, inplace=True)\n",
    "    data.dropna(subset=['clean_comments'], inplace=True)\n",
    "\n",
    "    #removing urls and html tags\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub(r'https?://\\S+www\\.\\S+', ' ', x))\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub(r'<.*?>', ' ', x))\n",
    "\n",
    "    #removing punctuations\n",
    "    punctuation = string.punctuation\n",
    "    data['clean_comments'] = data['clean_comments'].astype(str)\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: x.translate(str.maketrans('','',punctuation)))\n",
    "\n",
    "    #removing stopwords\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: \" \".join([w for w in x.split() if w not in STOPWORDS]))\n",
    "\n",
    "    #remove non english words\n",
    "    #WORDS = set(words.words())\n",
    "    #data['clean_comments'] = data['clean_comments'].apply(lambda x: \" \".join(w for w in x.split() if w in WORDS))\n",
    "\n",
    "    #removing special characters\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub('[^a-zA-Z0-9]', ' ', x))\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: re.sub('\\s+', ' ', x))\n",
    "\n",
    "    #stemization\n",
    "    #ps = PorterStemmer()\n",
    "    #data['clean_comments'] = data['clean_comments'].apply(lambda x: \" \".join([ps.stem(w) for w in x.split()]))\n",
    "\n",
    "    #lemmatization and pos tagging\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\":wordnet.NOUN,\"V\":wordnet.VERB,\"J\":wordnet.ADJ,\"R\":wordnet.ADV}\n",
    "    data['clean_comments'] = data['clean_comments'].apply(lambda x: \" \".join([lemmatizer.lemmatize(w,wordnet_map.get(pos[0],wordnet.NOUN)) for w, pos in pos_tag(x.split())]))\n",
    "    \n",
    "    #removing empty columns\n",
    "    data['clean_comments'].replace('', np.nan, inplace=True)\n",
    "    data.dropna(subset=['clean_comments'], inplace=True)\n",
    "\n",
    "    #removing null value\n",
    "    #data['clean_comments'].dropna(how = any, axis = 0)\n",
    "\n",
    "    #word tokanize\n",
    "    #data['clean_comments'] = data['clean_comments'].apply(lambda x: word_tokenize(x))\n",
    "    #data\n",
    "    \n",
    "    #using globle variable i and modifying it after saving into csv\n",
    "    global y\n",
    "    data.to_csv(\"C:\\\\Users\\\\Madan\\\\Desktop\\\\projects\\\\sentimentanalysis\\\\clean_comments\\\\\"+str(y)+\"clean.csv\")\n",
    "    y = y+1\n",
    "    print(data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 0\n",
    "\n",
    "for i in range(1):\n",
    "    data = pd.read_csv(\"C:\\\\Users\\\\Madan\\\\Desktop\\\\projects\\\\sentimentanalysis\\\\raw_comments\\\\\"+str(i)+\"raw.csv\", index_col=[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b9de5f633baef838eda09087c377b407887e2414fdad0aa392eeb39aea32887"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
